from pathlib import Path
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import json

BASE_URL = "https://minjust.gov.ru/ru/pages/politicheskie-partii/"

def normalize_url(url: str) -> str:
    """Нормализация URL: делает абсолютные ссылки, убирает лишние параметры"""
    if not url or url == "Нет документа":
        return None
    
    # Если ссылка относительная
    if url.startswith('/'):
        return urljoin(BASE_URL, url)
    
    # Если ссылка без протокола
    if url.startswith('//'):
        return f"https:{url}"
    
    # Если ссылка уже абсолютная
    if url.startswith('http'):
        return url
    
    # Для других случаев тоже делаем абсолютную
    return urljoin(BASE_URL, url)

def parse_political_parties(html_content: str):
    """Парсит HTML с политическими партиями"""
    soup = BeautifulSoup(html_content, 'lxml')
    parties = []
    
    # Сначала попробуем найти таблицу с партиями (основной способ)
    tables = soup.find_all('table')
    
    for table in tables:
        # Ищем строки таблицы (пропускаем заголовки)
        rows = table.find_all('tr')[1:]  # Пропускаем первую строку с заголовками
        
        for row in rows:
            try:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    # Ищем название партии (обычно в первой или второй колонке)
                    name = None
                    doc_url = None
                    
                    # Пробуем найти название партии
                    for i, cell in enumerate(cells[:3]):  # Проверяем первые 3 колонки
                        # Ищем текст без цифр (номера) и не слишком короткий
                        text = cell.get_text(" ", strip=True)
                        if text and len(text) > 3 and not text.replace('.', '').isdigit():
                            # Проверяем, нет ли в этой же ячейке ссылки с документом
                            links_in_cell = cell.find_all('a')
                            for link in links_in_cell:
                                href = link.get('href', '')
                                link_text = link.get_text(strip=True)
                                
                                # Если ссылка ведет на документ
                                if href and ('.pdf' in href.lower() or 
                                           'document' in href.lower() or
                                           'doc' in link_text.lower() or
                                           'устав' in link_text.lower()):
                                    doc_url = href
                                
                                # Если это название партии (не документ)
                                elif link_text and len(link_text) > 3:
                                    name = link_text
                            
                            # Если не нашли ссылку с названием, берем текст ячейки
                            if not name:
                                name = text
                            break
                    
                    # Если не нашли документ в той же ячейке, ищем во всей строке
                    if name and not doc_url:
                        all_links = row.find_all('a')
                        for link in all_links:
                            href = link.get('href', '')
                            if href and ('.pdf' in href.lower() or 
                                       '/documents/' in href or
                                       'устав' in link.get_text(strip=True).lower()):
                                doc_url = href
                                break
                    
                    if name:
                        parties.append({
                            "name": name[:200],  # Ограничиваем длину
                            "doc_url": normalize_url(doc_url) if doc_url else None
                        })
                        
            except Exception as e:
                # Пропускаем проблемные строки
                continue
    
    # Если не нашли в таблицах, ищем другие структуры
    if not parties:
        # Ищем элементы с названиями партий
        potential_items = soup.find_all(['li', 'div', 'p'])
        
        for item in potential_items:
            text = item.get_text(" ", strip=True)
            if text and len(text) > 20 and any(keyword in text.lower() 
                                             for keyword in ['партия', 'объединение', 'движение']):
                # Ищем название (первая строка или часть до запятой)
                name = text.split(',')[0].split('.')[0].strip()
                
                # Ищем ссылку на документ
                doc_url = None
                links = item.find_all('a')
                for link in links:
                    href = link.get('href', '')
                    if href and ('.pdf' in href.lower() or '/documents/' in href):
                        doc_url = href
                        break
                
                if name:
                    parties.append({
                        "name": name[:200],
                        "doc_url": normalize_url(doc_url) if doc_url else None
                    })
    
    return parties

def main():
    """Основная функция программы"""
    html_path = Path("politicheskie-partii.html")
    
    if not html_path.is_file():
        raise SystemExit(f"Файл {html_path.name} не найден. Скачайте страницу с {BASE_URL}/ru/pages/politicheskie-partii/")
    
    # Читаем HTML из файла
    html_content = html_path.read_text(encoding="utf-8", errors="ignore")
    
    # Парсим данные
    print("Парсим данные о политических партиях...")
    parties = parse_political_parties(html_content)
    
    # Выводим результат
    print(json.dumps(parties, ensure_ascii=False, indent=2))
    print(f"\nВсего партий: {len(parties)}")
    
    # Сохраняем в файл
    output_path = Path("parties.json")
    output_path.write_text(json.dumps(parties, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"\n[INFO] Результат сохранён в {output_path}")
    
    # Статистика
    with_docs = sum(1 for p in parties if p['doc_url'])
    without_docs = len(parties) - with_docs
    print(f"\nСтатистика:")
    print(f"  Партий с документами: {with_docs}")
    print(f"  Партий без документов: {without_docs}")

if __name__=="__main__":
    main()
